# vLLM Model Serving Configurations for H100 (80GB)
# Each config is a standalone vllm serve command with recommended kwargs.
# Adjust tensor-parallel-size, max-model-len, and gpu-memory-utilization for your cluster.

# ==============================================================================
# GLM-4.7 (THUDM/Zhipu AI) — 9B dense, MTP speculative decoding
# FP8 variant recommended for cost efficiency with minimal accuracy loss.
# ==============================================================================

# GLM-4.7 FP8 — 4×H100, MTP speculative decoding
# Total params: ~9B | Context: 128K | Architecture: Dense + MTP layers
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-glm-4-7
  namespace: ml-serving
  labels:
    app: vllm-glm-4-7
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-glm-4-7
  template:
    metadata:
      labels:
        app: vllm-glm-4-7
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          args:
            - --model
            - zai-org/GLM-4.7-FP8
            - --dtype
            - bfloat16
            - --tensor-parallel-size
            - "4"
            - --gpu-memory-utilization
            - "0.92"
            - --max-model-len
            - "65536"
            - --enable-prefix-caching
            # MTP speculative decoding (built-in draft layers)
            - --speculative-config.method
            - mtp
            - --speculative-config.num_speculative_tokens
            - "1"
            # Tool calling support
            - --enable-auto-tool-choice
            - --tool-call-parser
            - glm47
            - --reasoning-parser
            - glm45
            - --port
            - "8000"
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          resources:
            requests:
              cpu: "8"
              memory: 32Gi
              nvidia.com/gpu: "4"
            limits:
              nvidia.com/gpu: "4"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 180
            periodSeconds: 30
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: hf-model-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

# ==============================================================================
# GLM-4.5-Air FP8 — 8×H100, larger GLM variant
# ==============================================================================
# For BF16 use: zai-org/GLM-4.5-Air (requires more VRAM)
# For other GLM-4.x models: GLM-4.5, GLM-4.6, GLM-4.7-Flash
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-glm-4-5-air
  namespace: ml-serving
  labels:
    app: vllm-glm-4-5-air
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-glm-4-5-air
  template:
    metadata:
      labels:
        app: vllm-glm-4-5-air
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          args:
            - --model
            - zai-org/GLM-4.5-Air-FP8
            - --dtype
            - bfloat16
            - --tensor-parallel-size
            - "8"
            - --gpu-memory-utilization
            - "0.92"
            - --max-model-len
            - "65536"
            - --enable-prefix-caching
            - --enable-auto-tool-choice
            - --tool-call-parser
            - glm45
            - --reasoning-parser
            - glm45
            - --port
            - "8000"
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          resources:
            requests:
              cpu: "16"
              memory: 64Gi
              nvidia.com/gpu: "8"
            limits:
              nvidia.com/gpu: "8"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 180
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 240
            periodSeconds: 30
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: hf-model-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

# ==============================================================================
# MiniMax-M1 (MiniMaxAI) — 456B total / 45.9B active MoE
# Hybrid architecture: MoE + Lightning Attention for long context.
# Requires V0 engine (VLLM_USE_V1=0) and experts_int8 quantization.
# ==============================================================================

# MiniMax-M1-40k — 8×H100, INT8 expert quantization
# Total params: 456B | Active params: 45.9B | Context: 40K variant
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-minimax-m1
  namespace: ml-serving
  labels:
    app: vllm-minimax-m1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-minimax-m1
  template:
    metadata:
      labels:
        app: vllm-minimax-m1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          args:
            - --model
            - MiniMaxAI/MiniMax-M1-40k
            - --dtype
            - bfloat16
            - --tensor-parallel-size
            - "8"
            - --trust-remote-code
            - --quantization
            - experts_int8
            - --gpu-memory-utilization
            - "0.92"
            - --max-model-len
            - "4096"
            - --port
            - "8000"
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: VLLM_USE_V1
              value: "0"
            - name: SAFETENSORS_FAST_GPU
              value: "1"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          resources:
            requests:
              cpu: "16"
              memory: 128Gi
              nvidia.com/gpu: "8"
            limits:
              nvidia.com/gpu: "8"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 300
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 360
            periodSeconds: 30
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: hf-model-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

# ==============================================================================
# MiniMax-M1-80k — same as above but 80K context variant
# Increase max-model-len for longer context; may need reduced batch size.
# ==============================================================================
# To use: replace MiniMax-M1-40k with MiniMax-M1-80k above, and set:
#   --max-model-len 8192  (or higher if memory allows)

# ==============================================================================
# Kimi-K2 (Moonshot AI) — 1T total / 32B active MoE
# MLA attention, 384 experts (8 selected + 1 shared per token).
# Minimum deployment: 16×H100 (TP=8 + PP=2 across 2 nodes).
# ==============================================================================

# Kimi-K2 — TP+PP mode, 16×H100 (2 nodes × 8 GPUs), FP8 quantization
# Total params: 1T | Active params: 32B | Context: 128K | Vocab: 160K
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-kimi-k2
  namespace: ml-serving
  labels:
    app: vllm-kimi-k2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-kimi-k2
  template:
    metadata:
      labels:
        app: vllm-kimi-k2
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          args:
            - --model
            - moonshotai/Kimi-K2-Instruct
            - --dtype
            - bfloat16
            - --quantization
            - fp8
            - --kv-cache-dtype
            - fp8
            - --trust-remote-code
            - --tensor-parallel-size
            - "8"
            - --pipeline-parallel-size
            - "2"
            - --gpu-memory-utilization
            - "0.90"
            - --max-model-len
            - "65536"
            - --enable-chunked-prefill
            - --enable-prefix-caching
            # Tool calling support
            - --enable-auto-tool-choice
            - --tool-call-parser
            - kimi_k2
            - --port
            - "8000"
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          resources:
            requests:
              cpu: "32"
              memory: 256Gi
              nvidia.com/gpu: "8"
            limits:
              nvidia.com/gpu: "8"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 300
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 420
            periodSeconds: 30
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: hf-model-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

# NOTE: Kimi-K2 with TP+PP requires multi-node Ray cluster.
# The Deployment above runs on a single node with 8 GPUs but needs
# pipeline-parallel-size=2, which requires a second node in the Ray cluster.
# For single-node 8×H100: reduce max-model-len significantly or use
# DP+EP mode instead (see below).

# ==============================================================================
# Kimi-K2 — DP+EP mode (Data Parallel + Expert Parallel), 16×H100
# Higher throughput for batched workloads. Requires DeepEP/DeepGEMM.
# ==============================================================================
# Node 0 (primary):
#   vllm serve moonshotai/Kimi-K2-Instruct \
#     --trust-remote-code \
#     --data-parallel-size 16 \
#     --data-parallel-size-local 8 \
#     --data-parallel-address $MASTER_IP \
#     --data-parallel-rpc-port $PORT \
#     --enable-expert-parallel \
#     --max-num-batched-tokens 8192 \
#     --max-num-seqs 256 \
#     --gpu-memory-utilization 0.85 \
#     --enable-auto-tool-choice \
#     --tool-call-parser kimi_k2
#
# Node 1 (headless):
#   vllm serve moonshotai/Kimi-K2-Instruct \
#     --headless \
#     --data-parallel-start-rank 8 \
#     --trust-remote-code \
#     --data-parallel-size 16 \
#     --data-parallel-size-local 8 \
#     --data-parallel-address $MASTER_IP \
#     --data-parallel-rpc-port $PORT \
#     --enable-expert-parallel \
#     --max-num-batched-tokens 8192 \
#     --max-num-seqs 256 \
#     --gpu-memory-utilization 0.85 \
#     --enable-auto-tool-choice \
#     --tool-call-parser kimi_k2
