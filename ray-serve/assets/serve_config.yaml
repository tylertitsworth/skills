# Ray Serve LLM — vLLM multi-model deployment config.
# Deploy with: serve deploy serve_config.yaml
# Or embed in a RayService CRD's serveConfigV2 field.
#
# Uses ray.serve.llm (LLMConfig + build_openai_app) to deploy
# vLLM-backed models behind an OpenAI-compatible API.
# Docs: https://docs.ray.io/en/latest/serve/llm/serving-llms.html

applications:
  - name: llm_app
    route_prefix: /
    import_path: ray.serve.llm:build_openai_app
    args:
      llm_configs:
        # Chat model — tensor-parallel across 2 GPUs
        - model_loading_config:
            model_id: llama-8b
            model_source: meta-llama/Llama-3.1-8B-Instruct
          accelerator_type: A100
          deployment_config:
            autoscaling_config:
              min_replicas: 1
              max_replicas: 8
              target_ongoing_requests: 16
              upscale_delay_s: 30
              downscale_delay_s: 300
          engine_kwargs:
            tensor_parallel_size: 2
            gpu_memory_utilization: 0.90
            max_model_len: 8192
            enable_chunked_prefill: true
            enable_prefix_caching: true
          runtime_env:
            env_vars:
              VLLM_USE_V1: "1"

        # Embedding model — single GPU, high throughput
        - model_loading_config:
            model_id: gte-large
            model_source: Alibaba-NLP/gte-Qwen2-1.5B-instruct
          accelerator_type: A100
          deployment_config:
            autoscaling_config:
              min_replicas: 1
              max_replicas: 4
              target_ongoing_requests: 64
          engine_kwargs:
            gpu_memory_utilization: 0.85
            max_model_len: 4096
