# verl GRPO training configuration.
# For reasoning model training (e.g., math, code).
# Adjust model paths, GPU counts, and batch sizes for your cluster.
#
# Usage: python3 -m verl.trainer.main_ppo --config grpo_config.yaml

data:
  train_files: /data/math_prompts.parquet
  val_files: /data/math_val.parquet
  max_prompt_length: 1024
  max_response_length: 2048

actor_rollout_ref:
  model:
    path: meta-llama/Llama-3.2-3B-Instruct
  actor:
    # GRPO-specific settings
    clip_ratio: 0.2
    entropy_coeff: 0.001
    # DAPO variant: asymmetric clipping for more exploration
    # clip_ratio_low: 0.2
    # clip_ratio_high: 0.28
    ppo_epochs: 1
    optim:
      lr: 1.0e-6
      weight_decay: 0.01
      warmup_ratio: 0.05
  rollout:
    name: vllm
    tensor_model_parallel_size: 1
    gpu_memory_utilization: 0.4
    # Generation parameters
    temperature: 1.0
    top_p: 1.0
    max_tokens: 2048
    n: 8  # number of responses per prompt for GRPO
  ref:
    # Reference model for KL penalty
    fsdp_config:
      param_offload: true  # offload ref model to CPU to save GPU memory

reward_model:
  # Rule-based reward function for math
  reward_fn: verl.utils.reward.math_reward
  # Or use a trained reward model:
  # model:
  #   path: /models/reward-model
  #   fsdp_config:
  #     param_offload: true

algorithm:
  adv_estimator: grpo  # GRPO advantage estimation
  kl_ctrl:
    type: fixed
    kl_coeff: 0.01

trainer:
  total_epochs: 3
  save_freq: 50
  test_freq: 25
  project_name: grpo-math
  experiment_name: llama-3.2-3b-grpo
  logger:
    - wandb
  default_local_dir: /checkpoints/grpo

resource_pool:
  # 8 GPUs total: 4 for actor/rollout, 4 for reference
  trainer:
    - name: actor_rollout
      gpu: 4
    - name: ref
      gpu: 4
