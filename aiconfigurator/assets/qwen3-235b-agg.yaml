# Qwen3-235B-A22B — Aggregated on 8× H100 SXM (80 GB), vLLM backend
# Target: TTFT ≤ 250ms, TPOT ≤ 12ms, ISL=2000, OSL=500
#
# 235B MoE (128 experts, 8 active → 22B active params).
# FP8 weights ≈ 235 GB total but expert weights are sparse — only active
# expert slices loaded per token. With FP8-block GEMM, fits TP=4 on 4× H100.
# Search TP=[4,8] — TP=4 allows 2 replicas on 8 GPUs for higher throughput.

exps:
  - qwen3_235b_agg

qwen3_235b_agg:
  mode: "patch"
  serving_mode: "agg"
  model_path: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
  total_gpus: 8
  system_name: "h100_sxm"
  backend_name: "vllm"
  isl: 2000
  osl: 500
  ttft: 250.0
  tpot: 12.0
  config:
    worker_config:
      gemm_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [4, 8]
      tp_list: [4, 8]
      pp_list: [1]
      dp_list: [1]
      moe_tp_list: [4, 8]
      moe_ep_list: [1]
