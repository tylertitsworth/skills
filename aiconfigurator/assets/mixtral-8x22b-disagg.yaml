# Mixtral 8x22B (141B MOE) — Disaggregated on 16× H200 SXM
# Target: TTFT ≤ 500ms, TPOT ≤ 20ms, ISL=4000, OSL=1000
# Smaller MOE model — 8 experts, 2 active per token.
# FP8-block for both GEMM and MOE.

exps:
  - mixtral_8x22b_disagg

mixtral_8x22b_disagg:
  mode: "patch"
  serving_mode: "disagg"
  model_path: "mistralai/Mixtral-8x22B-Instruct-v0.1"
  total_gpus: 16
  system_name: "h200_sxm"
  backend_name: "trtllm"
  isl: 4000
  osl: 1000
  ttft: 500.0
  tpot: 20.0
  config:
    prefill_worker_config:
      gemm_quant_mode: "fp8_block"
      moe_quant_mode: "fp8_block"
      kvcache_quant_mode: "float16"
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [2, 4]
      tp_list: [1, 2]
      pp_list: [1]
      dp_list: [1]
      moe_tp_list: [1]
      moe_ep_list: [2, 4]
    decode_worker_config:
      gemm_quant_mode: "fp8_block"
      moe_quant_mode: "fp8_block"
      kvcache_quant_mode: "float16"
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [2, 4]
      tp_list: [1, 2, 4]
      pp_list: [1]
      dp_list: [1, 2, 4]
      moe_tp_list: [1]
      moe_ep_list: [2, 4]
    replica_config:
      num_gpu_per_replica: [8, 16]
      max_gpu_per_replica: 16
      max_prefill_worker: 8
      max_decode_worker: 8
    advanced_tuning_config:
      prefill_max_batch_size: 1
      decode_max_batch_size: 256
