# Llama 3.1 405B — Disaggregated on 32× H100 SXM (80 GB), vLLM backend
# Target: TTFT ≤ 300ms, TPOT ≤ 10ms, ISL=4000, OSL=500
#
# 405B in bf16 ≈ 810 GB → needs TP=8 per worker on H100 80 GB (810/8 ≈ 101 GB, FP8 → ~51 GB/GPU).
# FP8-block GEMM halves weight memory, leaving ~29 GB/GPU for KV cache.
# Prefill: TP=8 (compute-bound, benefits from max parallelism)
# Decode: TP=4 or TP=8 (memory-bound, more KV cache headroom with fewer params/GPU at FP8)

exps:
  - llama_405b_disagg

llama_405b_disagg:
  mode: "patch"
  serving_mode: "disagg"
  model_path: "meta-llama/Llama-3.1-405B-Instruct"
  total_gpus: 32
  system_name: "h100_sxm"
  backend_name: "vllm"
  isl: 4000
  osl: 500
  ttft: 300.0
  tpot: 10.0
  config:
    prefill_worker_config:
      gemm_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"         # FP8 KV saves memory for large model
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [8]           # 405B needs 8 GPUs/worker minimum at FP8
      tp_list: [8]
      pp_list: [1]
      dp_list: [1]
    decode_worker_config:
      gemm_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [4, 8]        # TP=4 with FP8 fits (~51 GB model + KV)
      tp_list: [4, 8]
      pp_list: [1]
      dp_list: [1]
    replica_config:
      num_gpu_per_replica: [16, 24, 32]
      max_gpu_per_replica: 32
      max_prefill_worker: 4
      max_decode_worker: 8
    advanced_tuning_config:
      prefill_latency_correction_scale: 1.1
      decode_latency_correction_scale: 1.08
      prefill_max_batch_size: 1
      decode_max_batch_size: 128
