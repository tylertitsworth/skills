# Llama 3.1 70B — Disaggregated on 8× H100 SXM (80 GB), vLLM backend
# Target: TTFT ≤ 200ms, TPOT ≤ 10ms, ISL=4000, OSL=500
#
# 70B in bf16 ≈ 140 GB → TP=2 at FP8 (~70 GB → 35 GB/GPU, leaves 45 GB for KV).
# Good candidate for disagg: prefill at TP=2 for speed, decode at TP=1 or TP=2 for max KV cache.
# With FP8 weights + FP8 KV cache, TP=1 fits (70 GB model, need >10 GB KV → tight but works).

exps:
  - llama_70b_disagg

llama_70b_disagg:
  mode: "patch"
  serving_mode: "disagg"
  model_path: "meta-llama/Llama-3.1-70B-Instruct"
  total_gpus: 8
  system_name: "h100_sxm"
  backend_name: "vllm"
  isl: 4000
  osl: 500
  ttft: 200.0
  tpot: 10.0
  config:
    prefill_worker_config:
      gemm_quant_mode: "fp8_block"
      kvcache_quant_mode: "float16"     # FP16 KV for prefill (short-lived, quality)
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [2, 4]
      tp_list: [2, 4]
      pp_list: [1]
      dp_list: [1]
    decode_worker_config:
      gemm_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"         # FP8 KV for decode (doubles cache capacity)
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [1, 2]        # TP=1 with FP8 fits on single H100
      tp_list: [1, 2]
      pp_list: [1]
      dp_list: [1]
    replica_config:
      num_gpu_per_replica: [4, 6, 8]
      max_gpu_per_replica: 8
      max_prefill_worker: 4
      max_decode_worker: 4
    advanced_tuning_config:
      prefill_latency_correction_scale: 1.1
      decode_latency_correction_scale: 1.08
      prefill_max_batch_size: 1
      decode_max_batch_size: 256
