# Qwen3-235B-A22B — Disaggregated on 16× H100 SXM (80 GB), vLLM backend
# Target: TTFT ≤ 200ms, TPOT ≤ 10ms, ISL=4000, OSL=500
#
# 235B MoE (128 experts, 8 active → 22B active). GQA (64Q/4KV). 262K context.
# FP8 variant fits TP=4 per worker. Disagg benefits: independent scaling of
# prefill (compute-bound MoE routing) and decode (memory-bound KV access).
# With 16 GPUs: 2 replicas at TP=4 prefill + TP=4 decode.

exps:
  - qwen3_235b_disagg

qwen3_235b_disagg:
  mode: "patch"
  serving_mode: "disagg"
  model_path: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
  total_gpus: 16
  system_name: "h100_sxm"
  backend_name: "vllm"
  isl: 4000
  osl: 500
  ttft: 200.0
  tpot: 10.0
  config:
    prefill_worker_config:
      gemm_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [4, 8]
      tp_list: [4, 8]
      pp_list: [1]
      dp_list: [1]
      moe_tp_list: [4, 8]
      moe_ep_list: [1]
    decode_worker_config:
      gemm_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"
      fmha_quant_mode: "float16"
      comm_quant_mode: "half"
      num_gpu_per_worker: [4]
      tp_list: [4]
      pp_list: [1]
      dp_list: [1]
      moe_tp_list: [4]
      moe_ep_list: [1]
    replica_config:
      num_gpu_per_replica: [8, 12, 16]
      max_gpu_per_replica: 16
      max_prefill_worker: 4
      max_decode_worker: 4
    advanced_tuning_config:
      prefill_latency_correction_scale: 1.08
      decode_latency_correction_scale: 1.06
      prefill_max_batch_size: 2
      decode_max_batch_size: 192
