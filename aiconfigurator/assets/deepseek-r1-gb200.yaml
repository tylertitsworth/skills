# DeepSeek-R1 (671B MOE) — Disaggregated on 128× GB200 SXM
# Target: TTFT ≤ 1000ms, TPOT ≤ 20ms, ISL=8000, OSL=2000
# Long-context reasoning model. Higher ISL/OSL for chain-of-thought.
# MTP enabled with nextn=2 for speculative decoding.
# Wide EP enabled for large-scale MOE distribution.

exps:
  - deepseek_r1_gb200

deepseek_r1_gb200:
  mode: "patch"
  serving_mode: "disagg"
  model_path: "deepseek-ai/DeepSeek-R1"
  total_gpus: 128
  system_name: "gb200_sxm"
  backend_name: "trtllm"
  isl: 8000
  osl: 2000
  prefix: 500
  ttft: 1000.0
  tpot: 20.0
  enable_wideep: true
  config:
    nextn: 2
    nextn_accept_rates: [0.85, 0.3, 0, 0, 0]
    prefill_worker_config:
      gemm_quant_mode: "fp8_block"
      moe_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"
      fmha_quant_mode: "fp8"
      comm_quant_mode: "half"
      num_gpu_per_worker: [8]
      tp_list: [1]
      pp_list: [1]
      dp_list: [1]
      moe_tp_list: [1]
      moe_ep_list: [8]
    decode_worker_config:
      gemm_quant_mode: "fp8_block"
      moe_quant_mode: "fp8_block"
      kvcache_quant_mode: "fp8"
      fmha_quant_mode: "fp8"
      comm_quant_mode: "half"
      num_gpu_per_worker: [8]
      tp_list: [1]
      pp_list: [1]
      dp_list: [1, 2, 4, 8]
      moe_tp_list: [1]
      moe_ep_list: [8]
    replica_config:
      num_gpu_per_replica: [32, 48, 64, 96, 128]
      max_gpu_per_replica: 128
      max_prefill_worker: 32
      max_decode_worker: 32
    advanced_tuning_config:
      prefill_latency_correction_scale: 1.15
      decode_latency_correction_scale: 1.1
      prefill_max_batch_size: 1
      decode_max_batch_size: 512
